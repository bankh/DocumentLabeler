{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyP7eRxj4oqGRGGyjcHCWt+L"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"VXEkQ8RTdi9J"},"outputs":[],"source":["import json\n","import os\n","import sys\n","import cv2\n","import numpy as np\n","from copy import deepcopy\n"]},{"cell_type":"code","source":["def trans_poly_to_bbox(poly):\n","  x1 = np.min([p[0] for p in poly])\n","  x2 = np.max([p[0] for p in poly])\n","  x3 = np.min([p[1] for p in poly])\n","  x4 = np.max([p[1] for p in poly])\n","  return [x1, x2, x3, x4]\n"],"metadata":{"id":"thjqZ3Utdwvv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def get_outer_poly(bbox_list):\n","  x1 = min([bbox[0] for bbox in bbox_list])\n","  y1 = min([bbox[1] for bbox in bbox_list])\n","  x2 = max([bbox[2] for bbox in bbox_list])\n","  y2 = max([bbox[3] for bbox in bbox_list])\n","  return [[x1,y1], [x2,y1], [x2, y2], [x1, y2]]"],"metadata":{"id":"i-v3DM-YeBIl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def load_funsd_label(image_dir, anno_dir):\n","    imgs = os.listdir(image_dir)\n","    annos = os.listdir(anno_dir)\n","\n","    imgs = [img.replace(\".png\", \"\") for img in imgs]\n","    annos = [anno.replace(\".json\", \"\") for anno in annos]\n","\n","    fn_info_map = dict()\n","    for anno_fn in annos:\n","        res = []\n","        with open(os.path.join(anno_dir, anno_fn + \".json\"), \"r\") as fin:\n","            infos = json.load(fin)\n","            infos = infos[\"form\"]\n","            old_id2new_id_map = dict()\n","            global_new_id = 0\n","            for info in infos:\n","                if info[\"text\"] is None:\n","                    continue\n","                words = info[\"words\"]\n","                if len(words) <= 0:\n","                    continue\n","                word_idx = 1\n","                curr_bboxes = [words[0][\"box\"]]\n","                curr_texts = [words[0][\"text\"]]\n","                while word_idx < len(words):\n","                    # switch to a new link\n","                    if words[word_idx][\"box\"][0] + 10 <= words[word_idx - 1][\n","                            \"box\"][2]:\n","                        if len(\"\".join(curr_texts[0])) > 0:\n","                            res.append({\n","                                \"transcription\": \" \".join(curr_texts),\n","                                \"label\": info[\"label\"],\n","                                \"points\": get_outer_poly(curr_bboxes),\n","                                \"linking\": info[\"linking\"],\n","                                \"id\": global_new_id,\n","                            })\n","                            if info[\"id\"] not in old_id2new_id_map:\n","                                old_id2new_id_map[info[\"id\"]] = []\n","                            old_id2new_id_map[info[\"id\"]].append(global_new_id)\n","                            global_new_id += 1\n","                        curr_bboxes = [words[word_idx][\"box\"]]\n","                        curr_texts = [words[word_idx][\"text\"]]\n","                    else:\n","                        curr_bboxes.append(words[word_idx][\"box\"])\n","                        curr_texts.append(words[word_idx][\"text\"])\n","                    word_idx += 1\n","                if len(\"\".join(curr_texts[0])) > 0:\n","                    res.append({\n","                        \"transcription\": \" \".join(curr_texts),\n","                        \"label\": info[\"label\"],\n","                        \"points\": get_outer_poly(curr_bboxes),\n","                        \"linking\": info[\"linking\"],\n","                        \"id\": global_new_id,\n","                    })\n","                    if info[\"id\"] not in old_id2new_id_map:\n","                        old_id2new_id_map[info[\"id\"]] = []\n","                    old_id2new_id_map[info[\"id\"]].append(global_new_id)\n","                    global_new_id += 1\n","            res = sorted(\n","                res, key=lambda r: (r[\"points\"][0][1], r[\"points\"][0][0]))\n","            for i in range(len(res) - 1):\n","                for j in range(i, 0, -1):\n","                    if abs(res[j + 1][\"points\"][0][1] - res[j][\"points\"][0][1]) < 20 and \\\n","                          (res[j + 1][\"points\"][0][0] < res[j][\"points\"][0][0]):\n","                        tmp = deepcopy(res[j])\n","                        res[j] = deepcopy(res[j + 1])\n","                        res[j + 1] = deepcopy(tmp)\n","                    else:\n","                        break\n","            # re-generate unique ids\n","            for idx, r in enumerate(res):\n","                new_links = []\n","                for link in r[\"linking\"]:\n","                    # illegal links will be removed\n","                    if link[0] not in old_id2new_id_map or \\\n","                       link[1] not in old_id2new_id_map:\n","                        continue\n","                    for src in old_id2new_id_map[link[0]]:\n","                        for dst in old_id2new_id_map[link[1]]:\n","                            new_links.append([src, dst])\n","                res[idx][\"linking\"] = deepcopy(new_links)\n","\n","            fn_info_map[anno_fn] = res\n","\n","    return fn_info_map"],"metadata":{"id":"rP5s6a3lZr16"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def main():\n","  test_image_dir = \"train_data/FUNSD/testing_data/images/\"\n","  test_anno_dir = \"train_data/FUNSD/testing_data/annotations/\"\n","  test_output_dir = \"train_data/FUNSD/test.json\"\n","\n","  fn_info_map = load_funsd_label(test_image_dir, test_anno_dir)\n","  with open(test_output_dir, \"w\") as fout:\n","    for fn in fn_info_map:\n","      fout.write(fn+\".png\"+\"\\t\"+json.dumps(fn_info_map[fn], ensure_ascii=False)+\"\\n\")\n","\n","  train_image_dir = \"train_data/FUNSD/training_data/images/\"\n","  train_anno_dir = \"train_data/FUNSD/trianing_data/annotations/\"\n","  train_output_dir = \"train_data/FUNSD/train.json\"\n","\n","  fn_info_map = load_funsd_label(train_image_dir, train_anno_dir)\n","  with open(train_output_dir, \"w\") as fout:\n","    for fn in fn_info_map:\n","      fout(write(fn+\".png\"+\"\\t\"+json.dump(fn_info_map[fn], ensure_ascii=False)+\"\\n\")\n","  \n","  print(\"OK\")\n","  return\n","\n","if__name__ = \"__main__\":\n","  main()"],"metadata":{"id":"KkU4K724eVi4"},"execution_count":null,"outputs":[]}]}