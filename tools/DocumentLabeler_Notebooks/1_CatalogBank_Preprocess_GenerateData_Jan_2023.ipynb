{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":16219,"status":"ok","timestamp":1672626560441,"user":{"displayName":"Sinan Bank","userId":"01980871032505042890"},"user_tz":480},"id":"S07dlz6JX42w","outputId":"3b0e954a-bf8b-4c2b-e911-84114a305aac"},"outputs":[],"source":["from google.colab import drive\n","drive.mount('/content/gdrive')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":192797,"status":"ok","timestamp":1672626754880,"user":{"displayName":"Sinan Bank","userId":"01980871032505042890"},"user_tz":480},"id":"jJ914iYaX8Oa","outputId":"436f18e1-df16-4d80-9faa-233f467d335b"},"outputs":[],"source":["! wget -O mini.sh https://repo.anaconda.com/miniconda/Miniconda3-py38_4.8.2-Linux-x86_64.sh\n","! chmod +x mini.sh\n","! bash ./mini.sh -b -f -p /usr/local\n","! conda install -q -y jupyter\n","! conda install -q -y google-colab -c conda-forge\n","! python -m ipykernel install --name \"py38\" --user"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":155,"status":"ok","timestamp":1672626767855,"user":{"displayName":"Sinan Bank","userId":"01980871032505042890"},"user_tz":480},"id":"MEpAVf-gX9TZ","outputId":"27604d5c-c12e-4887-fe2d-1841eaea258b"},"outputs":[{"name":"stdout","output_type":"stream","text":["User Current Version:- 3.8.0 | packaged by conda-forge | (default, Nov 22 2019, 19:11:38) \n","[GCC 7.3.0]\n"]}],"source":["# Reload the web page and execute this cell\n","import sys\n","print(\"User Current Version:-\", sys.version)"]},{"cell_type":"code","execution_count":14,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":13296,"status":"ok","timestamp":1672626783132,"user":{"displayName":"Sinan Bank","userId":"01980871032505042890"},"user_tz":480},"id":"qhBjOYuvX-h9","outputId":"037d5f53-0679-48ea-e092-d680d7d6a909"},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: pip in /home/ubuntu/miniconda3/envs/documentlabeler/lib/python3.8/site-packages (23.1.2)\n"]}],"source":["! python -m pip install --upgrade pip\n","! python -m pip install -q --upgrade ipykernel==5.3.4\n","! python -m pip install -q --upgrade ipython==7.9.0"]},{"cell_type":"code","execution_count":23,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":12078,"status":"ok","timestamp":1672626806935,"user":{"displayName":"Sinan Bank","userId":"01980871032505042890"},"user_tz":480},"id":"WI4VWKHTX_vS","outputId":"5f3b57d4-8d68-4467-bc88-46683dbd7489"},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting pdfplumber==0.6.0\n","  Downloading pdfplumber-0.6.0.tar.gz (46 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.7/46.7 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n","\u001b[?25hCollecting pdfminer.six==20211012 (from pdfplumber==0.6.0)\n","  Downloading pdfminer.six-20211012-py3-none-any.whl (5.6 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m18.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n","\u001b[?25hRequirement already satisfied: Pillow>=8.4 in /home/ubuntu/miniconda3/envs/documentlabeler/lib/python3.8/site-packages (from pdfplumber==0.6.0) (9.5.0)\n","Requirement already satisfied: Wand>=0.6.7 in /home/ubuntu/miniconda3/envs/documentlabeler/lib/python3.8/site-packages (from pdfplumber==0.6.0) (0.6.11)\n","Requirement already satisfied: cryptography in /home/ubuntu/miniconda3/envs/documentlabeler/lib/python3.8/site-packages (from pdfminer.six==20211012->pdfplumber==0.6.0) (40.0.2)\n","Requirement already satisfied: chardet in /home/ubuntu/miniconda3/envs/documentlabeler/lib/python3.8/site-packages (from pdfminer.six==20211012->pdfplumber==0.6.0) (5.1.0)\n","Requirement already satisfied: cffi>=1.12 in /home/ubuntu/miniconda3/envs/documentlabeler/lib/python3.8/site-packages (from cryptography->pdfminer.six==20211012->pdfplumber==0.6.0) (1.15.1)\n","Requirement already satisfied: pycparser in /home/ubuntu/miniconda3/envs/documentlabeler/lib/python3.8/site-packages (from cffi>=1.12->cryptography->pdfminer.six==20211012->pdfplumber==0.6.0) (2.21)\n","Building wheels for collected packages: pdfplumber\n","  Building wheel for pdfplumber (setup.py) ... \u001b[?25ldone\n","\u001b[?25h  Created wheel for pdfplumber: filename=pdfplumber-0.6.0-py3-none-any.whl size=33664 sha256=a339a10f4b57d2bc0d5d16ec67295a8d8bac982221378f85c9962da0793dadb9\n","  Stored in directory: /home/ubuntu/.cache/pip/wheels/42/74/b8/c95f716311b9ee587844cd73381fc19079bccfc05110378c61\n","Successfully built pdfplumber\n","Installing collected packages: pdfminer.six, pdfplumber\n","  Attempting uninstall: pdfminer.six\n","    Found existing installation: pdfminer.six 20220524\n","    Uninstalling pdfminer.six-20220524:\n","      Successfully uninstalled pdfminer.six-20220524\n","  Attempting uninstall: pdfplumber\n","    Found existing installation: pdfplumber 0.5.0\n","    Uninstalling pdfplumber-0.5.0:\n","      Successfully uninstalled pdfplumber-0.5.0\n","Successfully installed pdfminer.six-20211012 pdfplumber-0.6.0\n","Requirement already satisfied: pdf2image in /home/ubuntu/miniconda3/envs/documentlabeler/lib/python3.8/site-packages (1.16.3)\n","Requirement already satisfied: pillow in /home/ubuntu/miniconda3/envs/documentlabeler/lib/python3.8/site-packages (from pdf2image) (9.5.0)\n","Requirement already satisfied: pyPDF2==2.0.0 in /home/ubuntu/miniconda3/envs/documentlabeler/lib/python3.8/site-packages (2.0.0)\n","Requirement already satisfied: typing-extensions in /home/ubuntu/miniconda3/envs/documentlabeler/lib/python3.8/site-packages (from pyPDF2==2.0.0) (4.5.0)\n","Requirement already satisfied: tqdm in /home/ubuntu/.local/lib/python3.8/site-packages (4.65.0)\n"]}],"source":["# ! python -m pip install pdfplumber\n","# ! python -m pip install pdf2image\n","# ! python -m pip install pyPDF2==2.12.1\n","# ! python -m pip install tqdm\n","! python -m pip install pdfplumber==0.6.0\n","! python -m pip install pdf2image\n","! python -m pip install pyPDF2==2.0.0\n","! python -m pip install tqdm"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":9320,"status":"ok","timestamp":1672626844617,"user":{"displayName":"Sinan Bank","userId":"01980871032505042890"},"user_tz":480},"id":"luu4LA2zYA8t","outputId":"a4ebf474-89c7-4f89-ca75-2c0740d472eb"},"outputs":[],"source":["! sudo apt-get install poppler-utils"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":864,"status":"ok","timestamp":1672626851621,"user":{"displayName":"Sinan Bank","userId":"01980871032505042890"},"user_tz":480},"id":"tIAI7jhOYCrc","outputId":"1b2f3869-433b-445a-b67a-2717e515032f"},"outputs":[{"name":"stdout","output_type":"stream","text":["/mnt/data/Data-Document/Data_CatalogBankPaper/McMasterCarr/mcmaster_125_all_pdfs\n"]}],"source":["# Refresh browser in case the change doesn't work\n","#%cd {folder of the catalog} # For the catalog of the interest\n","#%cd /content/gdrive/MyDrive/Colab Notebooks/CSU_PhD_Research/CatalogsImagePaper/Misumi    # For Misumi catalog processing\n","#%cd /content/gdrive/MyDrive/Colab Notebooks/CSU_PhD_Research/CatalogsImagePaper/Newark    # For Newark catalog processing\n","#%cd /content/gdrive/MyDrive/Colab Notebooks/CSU_PhD_Research/CatalogsImagePaper/Thorlabs  # For Thorlabs catalog processing\n","#%cd /content/gdrive/MyDrive/Colab Notebooks/CSU_PhD_Research/CatalogsImagePaper/Grainger\n","# %cd /content/gdrive/MyDrive/Colab Notebooks/CSU_PhD_Research/CatalogsImagePaper/8020\n","%cd /mnt/data/Data-Document/Data_CatalogBankPaper/McMasterCarr/mcmaster_125_all_pdfs/"]},{"cell_type":"code","execution_count":1,"metadata":{"id":"VacMGaSUYEBP"},"outputs":[],"source":["import multiprocessing\n","import argparse\n","import pdfplumber\n","import os\n","from tqdm import tqdm\n","from pdfminer.layout import LTChar, LTLine\n","import re\n","from collections import Counter\n","import pdf2image\n","import numpy as np\n","from PIL import Image, ImageDraw \n","\n","from PyPDF2 import PdfFileReader, PdfFileWriter\n","import time"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"Jj4zySbSYFbQ"},"outputs":[],"source":["def create_pdf_pages(pdf_file,numPages,output_dir):\n","  pdfName = pdf_file.replace('.pdf','_')\n","\n","  #output_dir = './' + pdfName + 'pdfs'              # Point the output directory\n","  # if not os.path.exists(output_dir):\n","  #   os.makedirs(output_dir)\n","\n","  pdfSource = PdfFileReader(open(pdf_file,\"rb\"))\n","  print('PDF source: ', pdfSource)\n","  if numPages == 'all':\n","    numPages = pdfSource.numPages\n","  \n","  for page in tqdm(range(numPages)):\n","    print('\\nGenerating the individual pages from main catalogue.\\n')\n","    # ipdb.set_trace(context=10)\n","    time.sleep(0.1)\n","    output = PdfFileWriter()\n","    output.addPage(pdfSource.getPage(page))\n","\n","    with open(output_dir+'/'+pdfName+\"%s.pdf\" % page, \"wb\") as outputStream:\n","      output.write(outputStream)\n","    del output\n","    outputStream.close"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"AU-77_PdYIaA"},"outputs":[],"source":["def within_bbox(bbox_bound, bbox_in):\n","  assert bbox_bound[0] <= bbox_bound[2]\n","  assert bbox_bound[1] <= bbox_bound[3]\n","  assert bbox_in[0] <= bbox_in[2]\n","  assert bbox_in[1] <= bbox_in[3]\n","\n","  x_left = max(bbox_bound[0], bbox_in[0])\n","  y_top = max(bbox_bound[1], bbox_in[1])\n","  x_right = min(bbox_bound[2], bbox_in[2])\n","  y_bottom = min(bbox_bound[3], bbox_in[3])\n","  if x_right < x_left or y_bottom < y_top:\n","    return False\n","  \n","  intersection_area = (x_right - x_left) * (y_bottom - y_top)\n","  bbox_in_area = (bbox_in[2] - bbox_in[0]) * (bbox_in[3] - bbox_in[1])\n","  if bbox_in_area == 0:\n","    return False\n","  \n","  iou = intersection_area / float(bbox_in_area)\n","  return iou > 0.98"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"_EGAMdwmYJy3"},"outputs":[],"source":["def clear_word_box(word,objs,regex_clean):\n","  # detects ... r'(?:\\.){2,}' check https://regex101.com/\n","  if re.search(regex_clean,word): \n","    # Clear the regex item from the word and equate to cleared_word\n","    cleared_word = re.sub(regex_clean,'',word)\n","    # Find the new bounding box of this word\n","    word_bbox = []\n","  else:\n","    print('Nothing to clear!')\n","  return cleared_word, word_bbox"]},{"cell_type":"code","execution_count":7,"metadata":{"id":"Xr3I4fh3YLfv"},"outputs":[],"source":["def find_index_of_all_char(s, char):\n","    return [i for i, ltr in enumerate(s) if ltr == char]\n","  \n","# Alternative implementation\n","# s = 'A...ABC...'\n","# c = '.'\n","# idx = []\n","# # Find the index of the character\n","# for i in range(len(s)):\n","#   s_temp = s\n","#   if c==s_temp[i]:\n","#     idx.append(i)"]},{"cell_type":"code","execution_count":8,"metadata":{"id":"OlJ3gbPHYNI3"},"outputs":[],"source":["# remove character bounding box from the word bounding box\n","def remove_char_bbox(word_bbox, char_bbox, char_counter, idx_char):\n","    # ipdb.set_trace(context=10)\n","    # print('word_bbox',word_bbox)\n","    # print('char_bbox',char_bbox)\n","    if idx_char == 0 and char_counter == 0: # if it is the first char\n","        word_bbox[0] = char_bbox[2]\n","    elif char_counter > 0 and char_counter < len(word_bbox): # the bound box is in the middle of the bounding box\n","        word_bbox[0] = char_bbox[2] \n","    elif idx_char > char_counter: # last char in the word\n","        word_bbox[2] = char_bbox[0]\n","    else:\n","        print('Something went wrong!')\n","    return word_bbox"]},{"cell_type":"code","execution_count":9,"metadata":{"id":"kH0tNmQ_YOcj"},"outputs":[],"source":["def annotate_all(word_text, word_bbox, anno_color, anno_img, tokens, \n","                 pdf_width, pdf_height, target_width, target_height,\n","                 pdf_file, output_dir):\n","    # Format word_bbox\n","    f_x0 = int(word_bbox[0] / pdf_width * target_width)\n","    f_y0 = int(word_bbox[1] / pdf_height * target_height)\n","    f_x1 = int(word_bbox[2] / pdf_width * target_width)\n","    f_y1 = int(word_bbox[3] / pdf_height * target_height)\n","    # Normalized units target_width/width (image/pdf width)\n","    word_bbbox = tuple([f_x0, f_y0, f_x1, f_y1])\n","\n","    # Plot Annotation\n","    x0, y0, x1, y1 = word_bbbox\n","    x0, y0, x1, y1 = int(x0), int(y0), int(x1), int(y1)\n","    if(x0>=pdf_width):\n","        x0 = pdf_width-1\n","    if(x1>=pdf_width):\n","        x1 = pdf_width-1\n","    if(y0>=pdf_height):\n","        y0=pdf_height-1\n","    if(y1>=pdf_height):\n","        y1=pdf_height-1\n","\n","    # anno_color = [255, 0, 0]                        # Red color for text\n","    for x in range(x0, x1-1):\n","        anno_img[x, y0] = anno_color\n","    for x in range(x0, x1-1):\n","        anno_img[x, y1] = anno_color\n","    for y in range(y0, y1-1):\n","        anno_img[x0,y] = anno_color\n","    for y in range(y0, y1-1):\n","        anno_img[x1,y] = anno_color\n","\n","    with Image.open(os.path.join(output_dir, pdf_file.replace('.pdf', '')+'_annoverlay.png')) as im:\n","        draw = ImageDraw.Draw(im)\n","        draw.rectangle(word_bbbox,outline=tuple(anno_color))\n","        # write to stdout\n","        im.save(os.path.join(output_dir, pdf_file.replace('.pdf','')+'_annoverlay.png'))\n","    \n","    word_bbbox = tuple([str(t) for t in word_bbbox])\n","    fontname = 'default'\n","    tokens.append((word_text,) + word_bbbox + (fontname,))\n","    return tokens, anno_img"]},{"cell_type":"code","execution_count":8,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":527,"status":"ok","timestamp":1672625542510,"user":{"displayName":"Sinan Bank","userId":"01980871032505042890"},"user_tz":480},"id":"fSJVzBbbYPwv","outputId":"14202a41-228a-41cc-a3db-b328c5e1691b"},"outputs":[],"source":["idx_counter = 6\n","test = '0.45'\n","regex_clean = r\"(?:\\.){2,}\" # Clear unwanted pattern from the word set (Check Reg 101 the pattern)\n","# Find the idx and string of deliniation patterns (move to a helper function)\n","obj_skip_idx_str = [(g.start(), g.group()) for g in re.compile(regex_clean).finditer(test)]\n","obj_skip_idx_start = [i[0] for i in obj_skip_idx_str]\n","obj_skip_idx_end = [i[0]+len(i[1]) for i in obj_skip_idx_str]\n","obj_skip_str = [i[1] for i in obj_skip_idx_str]\n","# Create a list of range of the deliniation patterns and unpack them (move to a helper function)\n","obj_range_list = [[*range(x,y,1)] for (x, y) in zip(obj_skip_idx_start,obj_skip_idx_end)]  \n","obj_combined_list = [y for x in obj_range_list for y in x]\n","\n","if idx_counter in obj_combined_list:\n","    print('Skipping object')"]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[],"source":["def create_data(idx, pdf_file, data_dir, output_dir):\n","    \n","    target_width = 762\n","    target_height = 1000\n","    unwanted_character = '.'    # remove unwanted character from the word (. used for delinieation in the catalog)\n","    regex_clean = r\"^\\.+|\\.(?!\\d)|\\.+$\"\n","\n","    image_path = os.path.join(data_dir.replace('_pdfs','_images'), f'{pdf_file.replace(\".pdf\",\"_ori.jpg\")}')\n","    if(os.path.exists(image_path)):\n","      print('\\nImage:', image_path,' exists.\\n')\n","      return\n","    \n","    else:\n","      print('\\nImage:'+data_dir.replace('_pdfs','_images')+'/'+pdf_file.replace('.pdf','_ori.jpg')+' needs to be annotated.\\n')\n","      \n","      # Convert pdf to image\n","      try:\n","          pdf_images = pdf2image.convert_from_path(os.path.join(data_dir, pdf_file),size=(target_width,target_height))\n","      except:\n","          print(\"Failed to convert.\")\n","          return\n","\n","      # Import pdf pages\n","      page_tokens = []\n","      try:\n","          pdf = pdfplumber.open(os.path.join(data_dir, pdf_file))\n","      except:\n","          print(\"Failed pdf generation.\")\n","          return\n","\n","      # Show the loop with a progress bar via tqdim\n","      for page_id in range(len(pdf.pages)): # This runs for single or multiple pages\n","          # print('\\n...Generating the annotations.')\n","\n","          # Create original image\n","          pdf_images[page_id].save(os.path.join(output_dir, pdf_file.replace('.pdf', '')+'_ori.jpg'))\n","          pdf_images[page_id].save(os.path.join(output_dir, pdf_file.replace('.pdf', '')+'_annoverlay.png'))\n","          \n","          # if os.path.exists(os.path.join(output_dir, pdf_file.replace('.pdf', '')+'_ori.jpg')):\n","          #   continue\n","          # Prepare Words/ Tokens\n","          tokens = []\n","          this_page = pdf.pages[page_id]\n","          pdf_width = int(this_page.width)\n","          pdf_height = int(this_page.height)\n","          words = this_page.extract_words(x_tolerance=1.75 , y_tolerance=1.65, use_text_flow=True)\n","          # words = this_page.extract_words(x_tolerance=1.77 , y_tolerance=1.77, use_text_flow=False)\n","          # anno_img = np.ones([int(this_page.width), int(this_page.height)] + [3], dtype=np.uint8) * 255 # in pdf dimensions\n","          anno_img = np.ones([int(pdf_width), int(pdf_height)] + [3], dtype=np.uint8) * 255 # in pdf dimensions\n","          \n","          # Processing words\n","          for word in words:\n","            skip_word = False\n","            # print('word',word['text'])\n","            # if os.path.exists(os.path.join(output_dir, pdf_file.replace('.pdf', '')+'_ori.jpg')):\n","            #   continue\n","            # Check the word for any unwanted patterns and skip if the word is build by it for example '....' as a word\n","            if re.search(regex_clean,word['text']):\n","              # print('word to skip',word['text'])\n","              if (word['text'].count(unwanted_character) == len(word['text'])):\n","                continue\n","\n","            word_bbox_org = [float(word['x0']), float(word['top']), float(word['x1']), float(word['bottom'])]    # Original bounding box to use in comparison\n","            word_bbox = [float(word['x0']), float(word['top']), float(word['x1']), float(word['bottom'])]        # Bounding box for words to change and later store\n","            word_bbbox = tuple([int(word['x0']), int(word['top']), int(word['x1']), int(word['bottom'])])\n","            \n","            # consequitivedots = re.compile(r'\\.{3,}')\n","            # word_text = consequitivedots.sub('.', word['text']).strip()\n","            # Clear a string of unwanted pattern regex_clean.      \n","            word_text = re.sub(r\"\\.{2,}\", '.', word['text']).strip()\n","            word_text = re.sub(regex_clean, '', word_text)                                                       # Clear a string of unwanted pattern regex_clean.\n","\n","            if re.search(regex_clean,word['text']): \n","              skip_word = True\n","              idx_counter = 0\n","              idx_reverse_counter = 0\n","              # Extract characters from the layout objects and create new set of objects\n","              objs = []\n","              #idx_char = find_index_of_all_char(word['text'],'.')       # Find the index of the character\n","              splitted_words = re.split(regex_clean,word['text'])       # Clear a string of unwanted characters\n","              \n","              # if splitted words has an empty string pop that\n","              if ('' in splitted_words) :\n","                splitted_words.pop(splitted_words.index(''))              # Pop the empty item from splitted_words\n","\n","              if (len(splitted_words) > 1):\n","                splitted_words_str = (\" \".join([str(item) for item in splitted_words]))\n","                splitted_words_start_end = [(m.start(0), m.end(0)) for m in re.finditer(' ', splitted_words_str)]\n","              \n","              # Find the idx and string of deliniation patterns (move to a helper function)\n","              obj_skip_idx_str = [(g.start(), g.group()) for g in re.compile(regex_clean).finditer(word['text'])]\n","              obj_skip_idx_start = [i[0] for i in obj_skip_idx_str]\n","              obj_skip_idx_end = [i[0]+len(i[1]) for i in obj_skip_idx_str]\n","              obj_skip_str = [i[1] for i in obj_skip_idx_str]\n","              # Create a list of range of the deliniation patterns and unpack them (move to a helper function)\n","              obj_range_list = [[*range(x,y,1)] for (x, y) in zip(obj_skip_idx_start,obj_skip_idx_end)]  \n","              obj_combined_list = [y for x in obj_range_list for y in x]\n","\n","              # print('-') # Marker for the debugging point for the new word\n","              \n","              for splitted_word in splitted_words:\n","                if splitted_word == '':\n","                  continue\n","                first_char_marked = True\n","                char_counter = 0\n","                \n","                \n","                if (len(splitted_words) > 1):\n","                  # Splitted word check\n","                  if splitted_words.index(splitted_word)==0:\n","                    start_idx_splitted_word_char = 0\n","                    end_idx_splitted_word_char = start_idx_splitted_word_char + len(splitted_word) - 1\n","                  else:\n","                    start_idx_splitted_word_char = splitted_words_start_end[splitted_words.index(splitted_word)-1][0] # Starting index of splitted words\n","                    end_idx_splitted_word_char = start_idx_splitted_word_char + len(splitted_word) - 1\n","                else:\n","                  start_idx_splitted_word_char = 0\n","                  end_idx_splitted_word_char = start_idx_splitted_word_char + len(splitted_word) - 1\n","                  \n","                count_down = start_idx_splitted_word_char\n","\n","                for obj in this_page.layout._objs: # In efficient but works\n","                  \n","                  if not isinstance(obj, LTChar) or obj.get_text() == ' ':\n","                    continue\n","                  \n","                  obj_bbox = (obj.bbox[0], float(this_page.height) - obj.bbox[3],\n","                              obj.bbox[2], float(this_page.height) - obj.bbox[1])\n","\n","                  if within_bbox(word_bbox_org, obj_bbox):\n","                    \n","                    # To map the obj start point to word start point\n","                    if (len(objs) > 0 and idx_reverse_counter != 0):\n","                      idx_reverse_counter -= 1\n","                      continue\n","                    if idx_counter in obj_combined_list:\n","                      idx_counter += 1\n","                      continue\n","\n","                    if char_counter == 0:                                 # Start of the word\n","                      word_bbox[0] = obj_bbox[0]                          # Adjust bbox x value\n","                    elif start_idx_splitted_word_char > 0 and char_counter == 0:\n","                      word_bbox[0] = obj_bbox[0]\n","                    elif char_counter == (len(splitted_word) - 1):        # End of the word\n","                      word_bbox[2] = obj_bbox[2]                          # adjust bbox x value\n","                    \n","                    if len(splitted_word) == 1:                           # Single character string\n","                      word_bbox[0] = obj_bbox[0]                          # Adjust bbox x value\n","                      word_bbox[2] = obj_bbox[2]\n","\n","                    idx_counter += 1\n","                    char_counter += 1\n","                    objs.append(obj)\n","\n","                    if char_counter == len(splitted_word):\n","                      # print('Cleared word:', splitted_word)\n","                      # print('Length of the splitted word:', len(splitted_word))\n","                      color = [255, 0, 0] # (RED)\n","                      tokens, anno_img = annotate_all(splitted_word, word_bbox, color, anno_img, tokens, \n","                                                      pdf_width, pdf_height, target_width, target_height,\n","                                                      pdf_file, output_dir)\n","                      idx_reverse_counter = idx_counter\n","                      break\n","            \n","              # Extract the name of the font from the object\n","              fontname = []\n","              for obj in objs:\n","                fontname.append(obj.fontname)\n","              if len(fontname) != 0:\n","                c = Counter(fontname)\n","                fontname, _ = c.most_common(1)[0]\n","              else:\n","                fontname = 'default'\n","                \n","            if(skip_word == False):\n","              color = [255, 0, 0] # (RED)\n","              tokens, anno_img = annotate_all(word_text, word_bbox, color, anno_img, tokens, \n","                                            pdf_width, pdf_height, target_width, target_height,\n","                                            pdf_file, output_dir)\n","\n","          for image in this_page.images:\n","              image_bbox = (float(image['x0']), \n","                            float(image['top']), \n","                            float(image['x1']), \n","                            float(image['bottom']))\n","\n","              word_text = '##LTFigure##'\n","              anno_color = [0, 255, 0] # (GREEN)\n","              tokens, anno_img = annotate_all(word_text, image_bbox, anno_color, anno_img, tokens, \n","                                              pdf_width, pdf_height, target_width, target_height,\n","                                              pdf_file, output_dir)\n","      \n","          anno_img = np.swapaxes(anno_img, 0, 1)\n","          anno_img = Image.fromarray(anno_img, mode='RGB')\n","          page_tokens.append((page_id, tokens, anno_img))\n","          anno_img.save(os.path.join(output_dir, pdf_file.replace('.pdf', '')+'_ann.jpg'))\n","\n","          with open(os.path.join(output_dir, pdf_file.replace('.pdf', '')+'_{}.txt'.format(str(page_id))),'w',encoding='utf8') as fp:\n","              for token in tokens:\n","                  fp.write('\\t'.join(token) + '\\n')"]},{"cell_type":"code","execution_count":14,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":655,"status":"ok","timestamp":1672626256731,"user":{"displayName":"Sinan Bank","userId":"01980871032505042890"},"user_tz":480},"id":"luwZLv5eQIwg","outputId":"21bd46ed-38d6-4583-cade-d56eb37e4a59"},"outputs":[{"name":"stdout","output_type":"stream","text":["/mnt/data/Data-Document/Data_CatalogBankPaper/McMasterCarr/Paper_Supporting_Test\n","/mnt/data/Data-Document/Data_CatalogBankPaper/McMasterCarr/Paper_Supporting_Test\n"]}],"source":["%cd /mnt/data/Data-Document/Data_CatalogBankPaper/McMasterCarr/Paper_Supporting_Test/\n","!pwd"]},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[],"source":["!rm -rf /mnt/data/Data-Document/Data_CatalogBankPaper/McMasterCarr/McMasterCarrData100_pdfs\n","!rm -rf /mnt/data/Data-Document/Data_CatalogBankPaper/McMasterCarr/McMasterCarrData100_pdfs_images"]},{"cell_type":"code","execution_count":15,"metadata":{},"outputs":[{"data":{"text/plain":["['mcmaster-125_3378_1014.pdf',\n"," 'mcmaster-125_3378_1029.pdf',\n"," 'mcmaster-125_3378_1000.pdf',\n"," 'mcmaster-125_3378_1001.pdf',\n"," 'mcmaster-125_3378_1002.pdf',\n"," 'mcmaster-125_3378_1003.pdf',\n"," 'mcmaster-125_3378_1004.pdf',\n"," 'mcmaster-125_3378_1005.pdf',\n"," 'mcmaster-125_3378_1006.pdf',\n"," 'mcmaster-125_3378_1007.pdf',\n"," 'mcmaster-125_3378_1008.pdf',\n"," 'mcmaster-125_3378_1009.pdf',\n"," 'mcmaster-125_3378_1010.pdf',\n"," 'mcmaster-125_3378_1011.pdf',\n"," 'mcmaster-125_3378_1012.pdf',\n"," 'mcmaster-125_3378_1013.pdf',\n"," 'mcmaster-125_3378_1015.pdf',\n"," 'mcmaster-125_3378_1016.pdf',\n"," 'mcmaster-125_3378_1017.pdf',\n"," 'mcmaster-125_3378_1018.pdf',\n"," 'mcmaster-125_3378_1019.pdf',\n"," 'mcmaster-125_3378_1020.pdf',\n"," 'mcmaster-125_3378_1021.pdf',\n"," 'mcmaster-125_3378_1022.pdf',\n"," 'mcmaster-125_3378_1023.pdf',\n"," 'mcmaster-125_3378_1024.pdf',\n"," 'mcmaster-125_3378_1025.pdf',\n"," 'mcmaster-125_3378_1026.pdf',\n"," 'mcmaster-125_3378_1027.pdf',\n"," 'mcmaster-125_3378_1028.pdf',\n"," 'mcmaster-125_3378_1030.pdf',\n"," 'mcmaster-125_3378_1031.pdf',\n"," 'mcmaster-125_3378_1032.pdf',\n"," 'mcmaster-125_3378_1033.pdf',\n"," 'mcmaster-125_3378_1034.pdf',\n"," 'mcmaster-125_3378_1035.pdf',\n"," 'mcmaster-125_3378_1036.pdf',\n"," 'mcmaster-125_3378_1037.pdf',\n"," 'mcmaster-125_3378_1038.pdf',\n"," 'mcmaster-125_3378_1039.pdf',\n"," 'mcmaster-125_3378_1040.pdf',\n"," 'mcmaster-125_3378_1041.pdf',\n"," 'mcmaster-125_3378_1042.pdf',\n"," 'mcmaster-125_3378_1043.pdf',\n"," 'mcmaster-125_3378_1044.pdf',\n"," 'mcmaster-125_3378_1045.pdf',\n"," 'mcmaster-125_3378_1046.pdf',\n"," 'mcmaster-125_3378_1047.pdf',\n"," 'mcmaster-125_3378_1048.pdf',\n"," 'mcmaster-125_3378_1049.pdf',\n"," 'mcmaster-125_3378_1050.pdf']"]},"execution_count":15,"metadata":{},"output_type":"execute_result"}],"source":["os.listdir()"]},{"cell_type":"code","execution_count":16,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":429827,"status":"ok","timestamp":1672627611342,"user":{"displayName":"Sinan Bank","userId":"01980871032505042890"},"user_tz":480},"id":"Yawrqkj0YStC","outputId":"b999c050-a3f5-4aab-fcf9-d1c1631d4a4b"},"outputs":[{"ename":"FileNotFoundError","evalue":"[Errno 2] No such file or directory: 'mcmaster-125_3378.pdf'","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-16-68d171f8b417>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmakedirs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdatadir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;31m# STEP#1 Convert the catalog to an individual paged pdf document and save it\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0mcreate_pdf_pages\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcatalogName\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'.pdf'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'all'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdatadir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m   \u001b[0;31m# Extract the list of files and create a string array with .pdf suffix one\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-3-10a550e7dc9b>\u001b[0m in \u001b[0;36mcreate_pdf_pages\u001b[0;34m(pdf_file, numPages, output_dir)\u001b[0m\n\u001b[1;32m      6\u001b[0m   \u001b[0;31m#   os.makedirs(output_dir)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m   \u001b[0mpdfSource\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPdfFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpdf_file\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'PDF source: '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpdfSource\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mnumPages\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'all'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'mcmaster-125_3378.pdf'"]}],"source":["if __name__ == '__main__':\n","  # Generate data and catalog should be in the same directory\n","  catalogName = 'mcmaster-125_3378.pdf'\n","  # catalogName = '8020_Catalog_23_70.pdf'\n","  # catalogName = 'DigiKeyUKCatalogue.pdf'\n","  # catalogName = 'V21_1_Optomechanics.pdf'\n","  catalogName = catalogName.replace('.pdf','')\n","\n","  # Location of the folders\n","  datadir = './' + catalogName+'_all_pdfs'               # Enter where the pdf files are\n","  if not os.path.exists(datadir):\n","    os.makedirs(datadir)\n","    # STEP#1 Convert the catalog to an individual paged pdf document and save it\n","    create_pdf_pages(catalogName+'.pdf','all',datadir)\n","\n","  # Extract the list of files and create a string array with .pdf suffix one\n","  os.listdir(datadir) # For some reason the next line results in an error without running this line first\n","  pdf_folder = list(os.listdir(datadir))\n","  \n","  pdf_files = [t for t in pdf_folder if t.endswith('.pdf')] \n","  print('Number of pdf files:',len(pdf_files))\n","  output_dir = './' + catalogName + '_all_pdfs_images'         # Point the output directory\n","  # output_dir = './'\n","  if not os.path.exists(output_dir):\n","      os.makedirs(output_dir)\n","  \n","  # Reference for parallel processing:\n","  # https://stackoverflow.com/questions/8533318/multiprocessing-pool-when-to-use-apply-apply-async-or-map\n","  pool = multiprocessing.Pool(processes=10) ## Enable multiprocessor\n","  # print(\"Number of cpu : \", multiprocessing.cpu_count())\n","  print('\\n')\n","  # Show for loop progress with tqdm\n","  for idx, pdf_file in enumerate(tqdm(pdf_files)):\n","      basefilename = pdf_file.replace('.pdf', '')\n","      if not os.path.exists(output_dir):\n","        os.makedirs(output_dir) \n","      ori_jpg_file = os.path.join(output_dir, f'{basefilename}_ori.jpg')\n","      if os.path.exists(ori_jpg_file):\n","          # print(f\"\\n{ori_jpg_file} file exists for {pdf_file}\\n\")\n","          continue\n","        \n","      print(\"\\nData directory:\" + datadir)    \n","      print(\"Pdf File:\" + pdf_file)          \n","      # create_data(idx, pdf_file, datadir, output_dir)\n","      pool.apply_async(create_data, (idx, pdf_file, datadir, output_dir))    # multiprocessing\n","  \n","  pool.close()\n","  pool.join()"]}],"metadata":{"colab":{"authorship_tag":"ABX9TyN+u2DcnawyacrEl4RpgMQ0","machine_shape":"hm","provenance":[]},"gpuClass":"standard","kernelspec":{"display_name":"documentlabeler","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.0"}},"nbformat":4,"nbformat_minor":0}
